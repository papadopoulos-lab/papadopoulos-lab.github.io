{
  "hash": "00d16340b83db9d3568e0ed4ece795de",
  "result": {
    "markdown": "---\ntitle: \"Custom Airflow Docker Operator (Poor-Man's Kubernetes)\"\ndescription: |\n  Airflow is a platform to programmatically author, schedule, and monitor workflows data. Operators are the main building blocks that encapsulate logic to do a unit of work. We have created a custom Airflow operator that 1) checks a remote YAML file and 2) then takes a decision to do one of two actions. This allows us to create a \"poor-man's Kubernetes\".\nauthor:\n  - name: \"Richard Aubrey White\"\n    url: https://rwhite.no\n    affiliation: Folkehelseinstituttet\n    affiliation-url: https://www.fhi.no\n    orcid: 0000-0002-6747-1726\ndate: 2022-07-18\n---\n\n\n\n\n*This blog post has also been posted [here](https://docs.sykdomspulsen.no/posts/2022-07-18-custom-airflow-docker-operator/).*\n\n## What do we want?\n\nSykdomspulsen Analytics uses [Airflow](https://airflow.apache.org/) to schedule its tasks.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](airflow.png){width=100%}\n:::\n:::\n\n\nAirflow and Sykdomspulsen Analytics' tasks can be run on Kubernetes. This can be seen in the below graph, where one Airflow implementation dispatches tasks to both Server 1 and Server 2. However, with such a small team there is always the risk of something going wrong with a complicated Kubernetes setup. It is therefore preferable to have a failback solution that is independent of Kubernetes. We have achieved this by installing a duplicate Airflow system on each of the servers using Docker-compose. Each Docker-compose Airflow instance can dispatch tasks to its own server.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](kubernetes.png){width=100%}\n:::\n:::\n\n\nHowever, this means that we have anywhere between 1 to 3 duplicate Airflow DAGs running at any time. All of these will be reading and writing to the same databases. This is obviously not desirable. **We must have only one Airflow instance operative at any time.**\n\n## How do we get it?\n\nIt is not easy to seamlessly turn on and turn off multiple Airflow instances. We can, however, alter the operators inside each Airflow instance to be functional or non-functional.\n\nIt is for this reason that we developed a [custom Airflow Docker operator](https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html). This custom Airflow Docker operator:\n\n1.  Checks an external YAML config file (`https://raw.githubusercontent.com/USERNAME/REPO/main/airflow-dag-on-server.yaml`) to see which server each DAG should be run on.\n2.  Checks to see if this server is the correct one.\n3.  If this is not the correct server, change the command to be excecuted to: `echo NOT CORRECT SERVER`.\n4.  Executes the command inside the Docker container.\n\n## Details\n\n[**https://raw.githubusercontent.com/USERNAME/REPO/main/airflow-dag-on-server.yaml**](https://raw.githubusercontent.com/USERNAME/REPO/main/airflow-dag-on-server.yaml){.uri}\n\n``` yaml\n---\ndag_1: \"server-1.tld\"\n```\n\n**docker-compose.yml** (If using [Podman](https://podman.io/))\n\n``` yaml\n  volumes:\n    - /var/run/podman/podman.sock:/var/run/docker.sock\n  environment:\n    HOST_HOSTNAME: $HOSTNAME\n```\n\n**/opt/airflow/plugins/operators/sc_operators.py** accesses the environmental variable `HOST_HOSTNAME` that is passed through from `docker-compose.yml`.\n\n``` yaml\nimport os\nimport requests\nimport time\nimport yaml\n\nfrom airflow.operators.docker_operator import DockerOperator\n\nclass SCDockerOperator(DockerOperator):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def execute(self, context):\n        response = requests.get(\n            \"https://raw.githubusercontent.com/USERNAME/REPO/main/airflow-dag-on-server.yaml\",\n            headers = {\"Cache-Control\": \"no-cache\"}\n        )\n        unparsed_data = response.text\n    \n        data = yaml.safe_load(unparsed_data)\n        \n        print(data)\n        print(self.dag.dag_id)\n        \n        if self.dag.dag_id not in data:\n            self.command = '''bash -c \"echo NOT CORRECT SERVER\"'''\n        elif data[self.dag.dag_id] != os.getenv(\"HOST_HOSTNAME\"):\n            self.command = '''bash -c \"echo NOT CORRECT SERVER\"'''\n        else:\n            print(\"OK\")\n        \n        time.sleep(5)\n        retval = super().execute(context)\n        return retval\n```\n\n**/opt/airflow/dags/dag_1.py**\n\n``` yaml\nimport random\nfrom operators.sc_operators import SCDockerOperator\n\ntask = SCDockerOperator(\n    task_id='my_task',\n    image='localhost/splanalytics:latest',\n    container_name= 'my_task_' + str(random.randint(10,100000)),\n    api_version='auto',\n    auto_remove=True,\n    command=cmd,\n    docker_url='unix://var/run/docker.sock',\n    network_mode='bridge',\n    privileged = True,\n    dag=dag\n)\n```\n\n## Conclusion\n\nBy editing `https://raw.githubusercontent.com/USERNAME/REPO/main/airflow-dag-on-server.yaml` we can quickly choose which server will execute the desired task. This is an easy way to manually control identical installations of Airflow as a failback system.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}